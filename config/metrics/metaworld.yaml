# @package _group_
train:
  [
      # The format is [key, abbr, type, average/constant]
      # If a key ends with "_", n (= number of envs),
      # copies of the metric are created.
    [episode, E, int, average],
    [step, S, int, average],
    [duration, D, time, average],
    [episode_reward, R, float, average],
    [success, Su, float, average],
    [batch_reward, BR, float, average],
    [actor_loss, ALOSS, float, average],
    [critic_loss, CLOSS, float, average],
    [pig_critic_loss, PCLOSS, float, average],
    [pig_actor_loss, PALOSS, float, average],
    [expert_kl, EXPERT_KL, float, average],
    [weight_0,W_0,float,average],
    [weight_1,W_1,float,average],
    [weight_2,W_2,float,average],
    [ weight_3,W_3,float,average ],
    [ weight_4,W_4,float,average ],
    [ weight_5,W_5,float,average ],
    [ weight_6,W_6,float,average ],
    [ weight_7,W_7,float,average ],
    [ weight_8,W_8,float,average ],
    [ weight_9,W_9,float,average ],
    [ weight_10,W_10,float,average ],
    [ weight_11,W_11,float,average ],
    [ weight_12,W_12,float,average ],
    [ weight_13,W_13,float,average ],
    [ weight_14,W_14,float,average ],
    [ weight_15,W_15,float,average ],
    [ weight_16,W_11,float,average ],
    [ weight_17,W_12,float,average ],
    [ weight_18,W_13,float,average ],
    [ weight_19,W_14,float,average ],
    [ weight_20,W_15,float,average ],
    [ Q_weight,Q_WEIGHT,float,average ],
    [significance,SIG,float,average],
    [ae_loss, RLOSS, float, average],
    [ae_transition_loss, null, float, average],
    [reward_loss, null, float, average],
    [actor_target_entropy, null, float, average],
    [actor_entropy, null, float, average],
    [alpha_loss, null, float, average],
    [alpha_value, null, float, average],
    [contrastive_loss, MLOSS, float, average],
    [max_rat, MR, float, average],
    [env_index, ENV, str, constant],
    [episode_reward_env_index_, R_, float, average],
    [success_env_index_, Su_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, null, float, average],
    [critic_loss_agent_index_, AGENT_, float, average],
    [actor_distilled_agent_loss_agent_index_, null, float, average],
    [actor_loss_agent_index_, null, float, average],
    [actor_target_entropy_agent_index_, null, float, average],
    [actor_entropy_agent_index_, null, float, average],
    [alpha_loss_agent_index_, null, float, average],
    [alpha_value_agent_index_, null, float, average],
    [ae_loss_agent_index_, null, float, average],
  ]
eval:
  [
    [episode, E, int, average],
    [step, S, int, average],
    [episode_reward, R, float, average],
    [env_index, ENV, str, constant],
    [success, Su, float, average],
    [episode_reward_env_index_, R_, float, average],
    [success_env_index_, Su_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, AGENT_, float, average],
  ]
